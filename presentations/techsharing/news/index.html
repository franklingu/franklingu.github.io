---
layout: presentation_base
title: Summary of crawling news
meta_desc: Downloading, parsing and more with news -- understand and make sense of it.
---
<section>
  <img src="/img/python-logo.png" style="max-width: 250px; border-radius: 10px;">
  <h2>Summary of crawling news</h2>
  <h4>--downloading, parsing and more with news</h4>
  <p>
    <small>Created by <a href="http://franklingu.github.io">Junchao</a></small>
  </p>
</section>

<section>
  <h2>Today's speech's outline</h2>
  <ul>
    <li>Downloading news: requests, requests and more</li>
    <li>Parsing news: lxml vs BeautifulSoup</li>
    <li>All in one: newspaper</li>
    <li>What is more</li>
  </ul>
</section>

<section>
  <section>
    <h3>Downloading news: requests, requests and more requests</h3>
    <ul>
      <li>What about User-Agent</li>
      <li>What about Forms</li>
      <li>What about Cookies</li>
      <li>What about JavaScript</li>
      <li>What about Speed</li>
      <li>What about IP blocking</li>
    </ul>
  </section>
  <section>
    <h3>About User-Agent</h3>
    <ul>
      <li>curl or requests use headers of the underlying library</li>
      <li>we do not want to get noticed when crawling</li>
      <li>so we will usually fake User-Agent by customizing headers</li>
      <li><code>requests.get(url, headers={'User-Agent': 'Faked'})</code></li>
    </ul>
  </section>
  <section>
    <h3>About Forms</h3>
    <ul>
      <li>urlencoded or json or anything, it is just data</li>
      <li>so the ultimate way is to view requests sent by browser</li>
      <li>sometimes need a bit of reverse engineering: read and debug the JavaScript code</li>
      <li class="fragment">Chrome Dev Tools is THE best friend, love it!</li>
    </ul>
  </section>
  <section>
    <h3>About Cookies</h3>
    <ul>
      <li>Used for login traditionally</li>
      <li>with form submission: CSRF token</li>
      <li>the old way is a dict or CookieJar</li>
      <li>but seriously, with requests.Session, why you want to take the trouble by yourself?</li>
    </ul>
  </section>
  <section>
    <h3>About JavaScript</h3>
    <ul>
      <li>More and more websites using JavaScript</li>
      <li>if simple AJAX request only, view raw request and reverse engineer</li>
      <li>if some data manipulation, post-processing after downloading and parsing</li>
      <li>if SPA, reverse engineering could be hard -- possible however</li>
      <li>cont. can try selenium and pyvirtualdisplay</li>
      <li>cont. but I do not personally suggest it: unreliable in many ways</li>
    </ul>
  </section>
  <section>
    <h3>About Speed: threading</h3>
    <ul>
      <li>Yes Python has GIL, so what?</li>
      <li>Handles IO intensive tasks like downloading</li>
      <li>Detail: CPython, when doing read/write (or socket recv and send), will give up GIL</li>
      <li></li>
    </ul>
  </section>
  <section>
    <h3>About Speed: multiprocessing</h3>
    <ul>
      <li>Unix community: does not like threads and prefer processes --The Art of Unix Programming</li>
      <li>In the case of Python, communication by pickling and unpickling: another layer of slowness</li>
      <li>Used with threading: CPU intensive and IO intensive tasks could be solved</li>
    </ul>
  </section>
  <section>
    <h3>About Speed: hybrid</h3>
    <ul>
      <li>multiprocessing for multiple CPUs</li>
      <li>threading for IO work</li>
    </ul>
  </section>
  <section>
    <h3>About Speed: concurrent.futures (3k only)</h3>
    <ul>
      <li>Borrowed from Java</li>
      <li>Wrapper for user-friendliness</li>
      <li>Even more user-friendly than multiprocessing.Pool and threading</li>
    </ul>
  </section>
  <section>
    <h3>About Speed: gevent</h3>
    <ul>
      <li>coroutine-based methodology</li>
      <li>it is just like going back to Windows 95 minus</li>
      <li>involves monkey-patching, so potentially not safe</li>
      <li>see it with requests in grequests</li>
    </ul>
  </section>
  <section>
    <h3>About Speed: asyncio (3k only)</h3>
    <ul>
      <li>based on gevent and twisted</li>
      <li>even more complicated</li>
      <li>and potentially even faster and more scalable: <a href="https://github.com/channelcat/sanic">sanic</a></li>
    </ul>
  </section>
  <section>
    <h3>About IP blocking</h3>
    <ul>
      <li>Use cloud: request for new servers after a while</li>
      <li>Tor: restart a daemon client periodically</li>
    </ul>
  </section>
</section>

<section>
  <section>
    <h3>Parsing news: lxml vs BeautifulSoup</h3>
    <ul>
      <li>lxml</li>
      <li>BeautifulSoup4</li>
      <li>Differences and Comments</li>
    </ul>
  </section>
  <section>
    <h3>lxml: xml and html parsing</h3>
    <ul>
      <li>based on libxml2-dev libxslt-dev</li>
      <li>Python is slow, but it is using C</li>
      <li>known for being strict about html format</li>
      <li>improved as libxml2 as improved</li>
      <li>still not good at encoding though</li>
      <li>use BeautifulSoup for encoding detection</li>
    </ul>
  </section>
  <section>
    <h3>lxml: xpath and cssselect</h3>
    <ul>
      <li>xpath is powerful, but just a bit hard to understand compared to css query string</li>
      <li>use cssselect: lxml is using this package internally</li>
    </ul>
  </section>
  <section>
    <h3>lxml: pyquery</h3>
    <ul>
      <li>jquery is convenient to use, and somebody bring its query method to lxml</li>
    </ul>
  </section>
  <section>
    <h3>bs4: beautifulsoup4</h3>
    <ul>
      <li>first, bs4 and lxml can use each other</li>
      <li>for pure html.parser in bs4, it uses regex, more tolerent to broken html</li>
      <li>better as encoding detection</li>
      <li>slower than lxml, but it is generally fine. but</li>
      <li>one thing I really do not like: its select does not support all css features, not even nth-of-child</li>
    </ul>
  </section>
</section>

<section>
  <section>
    <h3>All in one: newspaper</h3>
    <ul>
      <li>The predecessor: python-goose and the ancestor: goose</li>
      <li>newspaper parse</li>
      <li>newspaper build</li>
      <li>newspaper nlp</li>
    </ul>
  </section>
  <section>
    <h3>python-goose and goose</h3>
    <ul>
      <li>goose: Scala-based article extractor, lack of development now</li>
      <li>python-goose: Python clone of goose, lack of developement now</li>
    </ul>
  </section>
  <section>
    <h3>newspaper</h3>
    <ul>
      <li>borrowed a lot of code for python goose</li>
      <li>build on top of requests and lxml</li>
      <li>uses Pillow for image extraction</li>
      <li>uses python-dateutil for datetime parsing</li>
      <li>uses nltk and jieba for NLP</li>
      <li>uses feedfinder2 and feedparser for link discovery</li>
    </ul>
  </section>
  <section>
    <h3>newspaper parse</h3>
    <ul>
      <li>title, authors, meta_language, publish_date, text, video, images ...</li>
      <li>What we can learn from it: title, publish_date, text</li>
      <li>What we may not need at all: video, images</li>
      <li>What is too general: authors, but we may not need this</li>
    </ul>
  </section>
  <section>
    <h3>newspaper build</h3>
    <ul>
      <li></li>
    </ul>
  </section>
  <section>
    <h3>newspaper nlp</h3>
    <ul>
      <li></li>
    </ul>
  </section>
</section>

<section>
    <h2 style="font-family: 'Pacifico', cursive;">Thank you</h2>
</section>
